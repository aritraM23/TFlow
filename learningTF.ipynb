{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learningTF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFlB+6l095UzQzSB2TZAB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aritraM23/TFlow/blob/main/learningTF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1WQowbYrl_d",
        "outputId": "cd39174d-6819-42c9-9e1a-46a50b84d58f"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#a tensor is an n-dim array of data\n",
        "# 1d : vector\n",
        "# 2d : matrix\n",
        "# etc.\n",
        "\n",
        "#initialisation of tensors\n",
        "x = tf.constant(4)\n",
        "# print(x) => tf.Tensor(4, shape=(), dtype=int32)...this has no shape\n",
        "#we can also specify the shape\n",
        "\n",
        "y = tf.constant(4, shape = (1,1))\n",
        "#print(y) => tf.Tensor([[4]], shape=(1, 1), dtype=int32)\n",
        "\n",
        "x = tf.ones((3,3))\n",
        "# print(x)\n",
        "\n",
        "# tf.Tensor(\n",
        "# [[1. 1. 1.]\n",
        "#  [1. 1. 1.]\n",
        "#  [1. 1. 1.]], shape=(3, 3), dtype=float32)..same will be case for tf.zeros\n",
        "\n",
        "# tf.eye(n) makes an identity matrix of dimension 1\n",
        "\n",
        "x = tf.random.normal((3,3), mean  = 0, stddev= 1)\n",
        "#above is for normal dist of values across tensors\n",
        "\n",
        "x = tf.random.uniform((1,3),minval = 0, maxval = 1)\n",
        "#gives tensor with random values from 0 to 1 \n",
        "\n",
        "x = tf.range(9) # => tf.Tensor([0 1 2 3 4 5 6 7 8], shape=(9,), dtype=int32)\n",
        "\n",
        "x = tf.range(start= 1, limit = 10,delta = 2) \n",
        "#print(x) => #tf.Tensor([1 3 5 7 9], shape=(5,), dtype=int32)\n",
        "\n",
        "#although type can be specified while initialisation, it can be casted too\n",
        "x = tf.cast(x, dtype = tf.float64)\n",
        "#print(x) -> tf.Tensor([1. 3. 5. 7. 9.], shape=(5,), dtype=float64)\n",
        "#tf.float(16,32,64), tf.int(8,16,32,64), tf.bool\n",
        "\n",
        "#mathematical operations\n",
        "x = tf.constant([1,2,3])\n",
        "y = tf.constant([9,8,7])\n",
        "\n",
        "z = tf.add(x,y) #-> tf.Tensor([10 10 10], shape=(3,), dtype=int32)\n",
        "#z = x+y also works\n",
        "#for subtraction, z = x-y or tf.subtract(x,y)\n",
        "\n",
        "#for division, elementwise division : tf.divide(x,y)\n",
        "#or z = x/y\n",
        "\n",
        "#for multiplication, z = tf.multiply(x,y)\n",
        "#or z = x*y\n",
        "\n",
        "z = tf.tensordot(x, y, axes = 1) #->tf.Tensor(46, shape=(), dtype=int32)\n",
        "#print(z)#this will do element wise multiplication and them add them\n",
        "#other way of achieving the same is:\n",
        "# z = tf.reduce_sum(x*y, axis = 0)\n",
        "# print(z) -> tf.Tensor(46, shape=(), dtype=int32)\n",
        "\n",
        "z = x ** 5 #element wise exponantiation\n",
        "#print(z) -> tf.Tensor([  1  32 243], shape=(3,), dtype=int32)\n",
        " \n",
        "x = tf.random.normal((2,3))\n",
        "y = tf.random.normal((3,4))\n",
        "z = tf.matmul(x,y) #matrix multiplication, it can also be done by using  z = x@y\n",
        "\n",
        "#indexing\n",
        "x = tf.constant([0,1,2,3,4,5])\n",
        "# print(x[:]) #this will print all the elements\n",
        "# print(x[1:]) #normal slicing rules, everything except first element\n",
        "# print(x[1:3])#prints from 1 to 3\n",
        "# #suppose we wanna skip one element alternatively\n",
        "# print(x[::2])\n",
        "print(x[::-1]) #reverse order\n",
        "\n",
        "#how to print a sub set of the array with desrired values\n",
        "indices = tf.constant([0,2])\n",
        "x_ind = tf.gather(x, indices)\n",
        "print(x_ind)\n",
        "\n",
        "x = tf.constant([[1,2],\n",
        "                [3,4]])\n",
        "\n",
        "print(x[0,1:]) #0th row and all columns except first\n",
        "\n",
        "#reshaping\n",
        "x = tf.range(9)\n",
        "\n",
        "x = tf.reshape(x , (3,3)) #reshaping\n",
        "print(x)\n",
        "\n",
        "x = tf.transpose(x, perm=[1,0])\n",
        "print(x)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([5 4 3 2 1 0], shape=(6,), dtype=int32)\n",
            "tf.Tensor([0 2], shape=(2,), dtype=int32)\n",
            "tf.Tensor([2], shape=(1,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[0 1 2]\n",
            " [3 4 5]\n",
            " [6 7 8]], shape=(3, 3), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[0 3 6]\n",
            " [1 4 7]\n",
            " [2 5 8]], shape=(3, 3), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SM40TC56tzp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAYDrH3uReHz",
        "outputId": "32b52f53-f0c5-44de-cc9a-58cdb74a621d"
      },
      "source": [
        "#building basic neural nets\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "#print(x_train.shape)\n",
        "#now we'd need to flatten them to have only one long column for those feature values\n",
        "#so we reshape it to same num of rows and change it to\n",
        "\n",
        "x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0 #this is being done to normalise the values of greyscale b/w 0 and 1, and float64 is being converted to float32 for ease in computation\n",
        "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "#Sequential API of keras(very convenient but not very flexible)\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "      #keras.Input(shape = (28*28)), #this sends a demo input to be able to let model print its summary\n",
        "     layers.Dense(512, activation = 'relu'),\n",
        "     layers.Dense(256, activation = 'relu'),\n",
        "     layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# print(model.summary()) #model.summary is a common debugging tool\n",
        "# import sys\n",
        "# sys.exit()\n",
        "\n",
        "# another way of adding layers is :\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(784)))\n",
        "model.add(layers.Dense(512, activation = 'relu'))\n",
        "model.add(layers.Dense(256, activation = 'relu'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "#extracting specific layer outputs:\n",
        "# model = keras.Model(inputs = model.inputs,\n",
        "#                     outputs = [model.layers[-1].output]) #index value decides which layer to take, -1 means last, -2 means second last and henceforth\n",
        "#if there are names of the layers by name argument, then they can be accessed in te=he following way:\n",
        "#[model.get_layer('name of the layer').output]\n",
        "\n",
        "#for all layers, outputs = [layer.output for layer in model.layers]\n",
        "#features = model.predict(x_train)\n",
        "#for feature in features:\n",
        "#   print(feature.shape)\n",
        "\n",
        "\n",
        "# feature = model.predict(x_train)\n",
        "# print(feature.shape)\n",
        "# import sys\n",
        "# sys.exit()\n",
        "\n",
        "#Functional API (A bit more flexible)\n",
        "inputs = keras.Input(shape = (784))\n",
        "x = layers.Dense(512, activation = 'relu')(inputs)\n",
        "x = layers.Dense(256, activation = 'relu')(x)\n",
        "outputs = layers.Dense(10, activation = 'softmax')(x)\n",
        "model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "\n",
        "#compile is about the specifications of the model\n",
        "model.compile(\n",
        "    #Use this crossentropy loss function when there are two or more label classes. \n",
        "    #We expect labels to be provided as integers.\n",
        "    #from_logits is true to send the last layer through a softmax activation first\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits= False), \n",
        "    optimizer = keras.optimizers.Adam(lr = 0.001),\n",
        "    metrics = [\"accuracy\"],\n",
        ")\n",
        "\n",
        "#fit is more about training and preparing it\n",
        "model.fit(x_train, y_train, batch_size = 32, epochs = 5, verbose = 2)\n",
        "model.evaluate(x_test, y_test, batch_size = 32, verbose = 2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 - 10s - loss: 0.1869 - accuracy: 0.9444\n",
            "Epoch 2/5\n",
            "1875/1875 - 9s - loss: 0.0811 - accuracy: 0.9750\n",
            "Epoch 3/5\n",
            "1875/1875 - 9s - loss: 0.0549 - accuracy: 0.9819\n",
            "Epoch 4/5\n",
            "1875/1875 - 9s - loss: 0.0407 - accuracy: 0.9868\n",
            "Epoch 5/5\n",
            "1875/1875 - 9s - loss: 0.0326 - accuracy: 0.9895\n",
            "313/313 - 1s - loss: 0.0762 - accuracy: 0.9795\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07616499066352844, 0.9794999957084656]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qZELZL4VVdL"
      },
      "source": [
        "#Convolutional Networks\n",
        "\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "#cifar10 is a dataset consisting of 10 classes of objects with 50k training images & 10k test images with 32*32 RGB\n",
        "e = math.e\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0 \n",
        "#this is being done to normalise the values of greyscale b/w 0 and 1, and float64 is being converted to float32 for ease in computation\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "#since it is convolutional so we're not going to flatten it\n",
        "#sequential api approach\n",
        "# model = keras.Sequential(\n",
        "#     [\n",
        "#       keras.Input(shape=(32,32,3)),\n",
        "#       layers.Conv2D(32,3, padding = 'valid', activation = 'relu'),\n",
        "#       layers.MaxPooling2D(pool_size = (2,2)),\n",
        "#       layers.Conv2D(64,3,activation = 'relu'),\n",
        "#       layers.MaxPooling2D(),\n",
        "#       layers.Conv2D(128, 3, activation = 'relu'),\n",
        "#       layers.Flatten(),\n",
        "#       layers.Dense(64,activation = 'relu'),\n",
        "#       layers.Dense(10),\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "#functional api, with batch normalisation\n",
        "def my_model():\n",
        "  #l2 is a batch normalisation method as well\n",
        "  inputs = keras.Input(shape=(32,32,3))\n",
        "  x = layers.Conv2D(\n",
        "      32,3, padding = 'same', kernel_regularizer = regularizers.l2(0.01),            \n",
        "                    )(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.MaxPooling2D()(x)\n",
        "  x = layers.Conv2D(\n",
        "      64, 3,padding = 'same', kernel_regularizer = regularizers.l2(0.01),\n",
        "      )(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Conv2D(\n",
        "      128,3,padding = 'same', kernel_regularizer = regularizers.l2(0.01),\n",
        "      )(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(\n",
        "      64 , activation='relu', kernel_regularizer = regularizers.l2(0.01),\n",
        "      )(x)\n",
        "  x = layers.Dropout(0.5)(x) #drops .5 part of the connections in b/w those layers\n",
        "  outputs = layers.Dense(10)(x)\n",
        "  model = keras.Model(inputs = inputs , outputs = outputs)\n",
        "  return model\n",
        "\n",
        "model = my_model()\n",
        "#print(model.summary())\n",
        "#when overfitting, we regularise the model by using dropout, early stoppage etc\n",
        "# for that we import regularizers from keras\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits= True),\n",
        "    optimizer = keras.optimizers.Adam(lr = 3*e - 4),\n",
        "    metrics = [\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=64,epochs = 10, verbose = 2)\n",
        "model.evaluate(x_test, y_test, batch_size= 64, verbose = 2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnX8W5kl8-7Z",
        "outputId": "b8fbe60f-db55-4580-db6e-3d43f97c86c8"
      },
      "source": [
        "#Recurring Neural Networks\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "e = math.e\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255.0 #this is being done to normalise the values of greyscale b/w 0 and 1, and float64 is being converted to float32 for ease in computation\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "#in the first timestamp it'll send the first row and in 2nd, second row shall be sent etc.\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape = (None, 28))) #28 timestamps, none is here because we don't need a specific timestamp...28 pixels for each timestamp\n",
        "model.add(\n",
        "    layers.SimpleRNN(256, return_sequences = True, activation = 'tanh') #multiple RNN layers on top of each other shall be stacked \n",
        ") #default activation is tanh\n",
        "model.add(layers.SimpleRNN(256, activation = 'tanh'))\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits= True),\n",
        "    optimizer = keras.optimizers.Adam(lr = 0.001),\n",
        "    metrics = [\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=64,epochs = 10, verbose = 2)\n",
        "model.evaluate(x_test, y_test, batch_size= 64, verbose = 2)\n",
        "\n",
        "#to build a GRU or LSTM, we just have to change SimpleRNN to GRU or LSTM and it will all be the same"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "938/938 - 91s - loss: 0.2962 - accuracy: 0.9109\n",
            "Epoch 2/10\n",
            "938/938 - 89s - loss: 0.1875 - accuracy: 0.9449\n",
            "Epoch 3/10\n",
            "938/938 - 89s - loss: 0.1682 - accuracy: 0.9516\n",
            "Epoch 4/10\n",
            "938/938 - 89s - loss: 0.1388 - accuracy: 0.9596\n",
            "Epoch 5/10\n",
            "938/938 - 90s - loss: 0.1401 - accuracy: 0.9600\n",
            "Epoch 6/10\n",
            "938/938 - 90s - loss: 0.1416 - accuracy: 0.9594\n",
            "Epoch 7/10\n",
            "938/938 - 90s - loss: 0.1279 - accuracy: 0.9644\n",
            "Epoch 8/10\n",
            "938/938 - 90s - loss: 0.1422 - accuracy: 0.9603\n",
            "Epoch 9/10\n",
            "938/938 - 90s - loss: 0.1200 - accuracy: 0.9654\n",
            "Epoch 10/10\n",
            "938/938 - 92s - loss: 0.1281 - accuracy: 0.9636\n",
            "157/157 - 5s - loss: 0.1111 - accuracy: 0.9679\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11112425476312637, 0.9678999781608582]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "tOu7StWz2l8D",
        "outputId": "cef66b3e-54cf-4dc0-b233-21a5ee260cd6"
      },
      "source": [
        "#Bidirectional LSTM\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "e = math.e\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255.0 #this is being done to normalise the values of greyscale b/w 0 and 1, and float64 is being converted to float32 for ease in computation\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "#in the first timestamp it'll send the first row and in 2nd, second row shall be sent etc.\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape = (None, 28))) #28 timestamps, none is here because we don't need a specific timestamp...28 pixels for each timestamp\n",
        "model.add(\n",
        "    layers.Bidirectional(\n",
        "      layers.LSTM(256, return_sequences = True, activation = 'tanh') #multiple RNN layers on top of each other shall be stacked \n",
        "    )     #since, it is bidriectional, we'll get 512 nodes instead of 256,\n",
        "    #one time it will be forward, other time it will be backwards hence doubled\n",
        " ) #default activation is tanh\n",
        "model.add(\n",
        "    layers.Bidirectional(\n",
        "      layers.LSTM(256, activation = 'tanh')\n",
        "    )\n",
        ")\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits= True),\n",
        "    optimizer = keras.optimizers.Adam(lr = 0.001),\n",
        "    metrics = [\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train,batch_size=64,epochs = 10, verbose = 2)\n",
        "model.evaluate(x_test, y_test, batch_size= 64, verbose = 2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0bf6867d6b20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "M5CrttEu7Uoe",
        "outputId": "daf905e5-1e15-49d5-eeb5-4e740e6b37ce"
      },
      "source": [
        "#depth of Functional API\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "from tensorflow.keras import layers,regularizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import pandas as pd\n",
        "\n",
        "# HYPERPARAMETERS\n",
        "BATCH_SIZE = 64\n",
        "WEIGHT_DECAY = 0.001\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Make sure we don't get any GPU errors\n",
        "# physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "train_images = os.getcwd() + \"/train_images/\" + train_df.iloc[:, 0].values\n",
        "test_images = os.getcwd() + \"/test_images/\" + test_df.iloc[:, 0].values\n",
        "\n",
        "train_labels = train_df.iloc[:, 1:].values\n",
        "test_labels = test_df.iloc[:, 1:].values\n",
        "\n",
        "\n",
        "def read_image(image_path, label):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_image(image, channels=1, dtype=tf.float32)\n",
        "\n",
        "    # In older versions you need to set shape in order to avoid error\n",
        "    # on newer (2.3.0+) the following 3 lines can safely be removed\n",
        "    image.set_shape((64, 64, 1))\n",
        "    label[0].set_shape([])\n",
        "    label[1].set_shape([])\n",
        "\n",
        "    labels = {\"first_num\": label[0], \"second_num\": label[1]}\n",
        "    return image, labels\n",
        "\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "train_dataset = (\n",
        "    train_dataset.shuffle(buffer_size=len(train_labels))\n",
        "    .map(read_image)\n",
        "    .batch(batch_size=BATCH_SIZE)\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "test_dataset = (\n",
        "    test_dataset.map(read_image)\n",
        "    .batch(batch_size=BATCH_SIZE)\n",
        "    .prefetch(buffer_size=AUTOTUNE)\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(64, 64, 1)) #64 by 64 pixels and 1 channl cause greyscale\n",
        "\n",
        "x = layers.Conv2D(\n",
        "    filters=32,\n",
        "    kernel_size=3,\n",
        "    padding=\"same\",\n",
        "    kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
        ")(inputs)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = keras.activations.relu(x)\n",
        "x = layers.Conv2D(64, 3, kernel_regularizer=regularizers.l2(WEIGHT_DECAY),)(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = keras.activations.relu(x)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "x = layers.Conv2D(\n",
        "    64, 3, activation=\"relu\", kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n",
        ")(x)\n",
        "x = layers.Conv2D(128, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D()(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "\n",
        "#here we have the outputs for 2 diff numbers in the image which isn't possible in Sequential\n",
        "output1 = layers.Dense(10, activation=\"softmax\", name=\"first_num\")(x)\n",
        "output2 = layers.Dense(10, activation=\"softmax\", name=\"second_num\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=[output1, output2])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(LEARNING_RATE),\n",
        "    loss=[\n",
        "      keras.losses.SparseCategoricalCrossentropy(),\n",
        "      keras.losses.SparseCategoricalCrossentropy(),\n",
        "    ],\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(train_dataset, epochs=5, verbose=2)\n",
        "model.evaluate(test_dataset, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-fb9ced5b57a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m:  /content/train_images/845_24.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_74303]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg_7p6TMC2Lq",
        "outputId": "e1c43d45-70d2-40e9-89c4-0e7d3a618b73"
      },
      "source": [
        "#Model Subclassing\n",
        "\n",
        "#increases flexibility\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "#print(x_train.shape)\n",
        "#now we'd need to flatten them to have only one long column for those feature values\n",
        "#so we reshape it to same num of rows and change it to\n",
        "\n",
        "x_train = x_train.reshape(-1, 28,28,1).astype(\"float32\") / 255.0 #this is being done to normalise the values of greyscale b/w 0 and 1, and float64 is being converted to float32 for ease in computation\n",
        "x_test = x_test.reshape(-1, 28,28,1).astype(\"float32\") / 255.0\n",
        "\n",
        "# CNN-> BatchNornm -> ReLU (common structure)\n",
        "# what if we write the structure in class because running timestamp wise would take high computational power\n",
        "\n",
        "#subclasing is the same way of using pytorch\n",
        "\n",
        "class CNNBlock(layers.Layer):\n",
        "  def __init__(self, out_channels, kernel_size = 3):\n",
        "    super(CNNBlock, self).__init__() #runs parent class layer by layer\n",
        "    self.conv = layers.Conv2D(out_channels, kernel_size, padding = 'same') #Conv layer\n",
        "    self.bn = layers.BatchNormalization() #BatchNorm layer\n",
        "\n",
        "  def call(self, input_tensor, training = False):\n",
        "    #call method is the forward method, which takes the input tensor and runs the layers\n",
        "    x = self.conv(input_tensor)\n",
        "    x = self.bn(x, training = training)\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "      CNNBlock(32),\n",
        "      CNNBlock(64),\n",
        "      CNNBlock(128),\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics = [\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size = 64, epochs = 3, verbose = 2)\n",
        "model.evaluate(x_test, y_test, batch_size = 64, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "938/938 - 738s - loss: 0.5051 - accuracy: 0.9501\n",
            "Epoch 2/3\n",
            "938/938 - 723s - loss: 0.0672 - accuracy: 0.9844\n",
            "Epoch 3/3\n",
            "938/938 - 728s - loss: 0.0318 - accuracy: 0.9902\n",
            "157/157 - 29s - loss: 0.0614 - accuracy: 0.9841\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.061383698135614395, 0.9840999841690063]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQRs9oTCPW_S",
        "outputId": "19316c6e-faa0-4ad3-de59-57f5e5068a3f"
      },
      "source": [
        "#Resnet\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "#print(x_train.shape)\n",
        "#now we'd need to flatten them to have only one long column for those feature values\n",
        "#so we reshape it to same num of rows and change it to\n",
        "\n",
        "x_train = x_train.reshape(-1, 28,28,1).astype(\"float32\") / 255.0 #this is being done to normalise the values of greyscale b/w 0 and 1, and float64 is being converted to float32 for ease in computation\n",
        "x_test = x_test.reshape(-1, 28,28,1).astype(\"float32\") / 255.0\n",
        "\n",
        "# CNN-> BatchNornm -> ReLU (common structure)\n",
        "# what if we write the structure in class because running timestamp wise would take high computational power\n",
        "\n",
        "#subclasing is the same way of using pytorch\n",
        "\n",
        "class CNNBlock(layers.Layer):\n",
        "  def __init__(self, out_channels, kernel_size = 3):\n",
        "    super(CNNBlock, self).__init__() #runs parent class layer by layer\n",
        "    self.conv = layers.Conv2D(out_channels, kernel_size, padding = 'same') #Conv layer\n",
        "    self.bn = layers.BatchNormalization() #BatchNorm layer\n",
        "\n",
        "  def call(self, input_tensor, training = False):\n",
        "    #call method is the forward method, which takes the input tensor and runs the layers\n",
        "    x = self.conv(input_tensor)\n",
        "    x = self.bn(x, training = training)\n",
        "    x = tf.nn.relu(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "#creating a residula network with 3 CNN blocks\n",
        "class ResBlock(layers.Layer):\n",
        "  def __init__(self, channels):\n",
        "    super(ResBlock, self).__init__()\n",
        "    self.cnn1 = CNNBlock(channels[0])\n",
        "    self.cnn2 = CNNBlock(channels[1])\n",
        "    self.cnn3 = CNNBlock(channels[2])\n",
        "    self.pooling = layers.MaxPooling2D()\n",
        "    self.identity_mapping = layers.Conv2D(channels[1], 1, padding = 'same')\n",
        "\n",
        "  def call(self, input_tensor, training = False):\n",
        "    x = self.cnn1(input_tensor, training = training)\n",
        "    x = self.cnn2(x, training = training)\n",
        "    x = self.cnn3(\n",
        "        x + self.identity_mapping(input_tensor), training = training,\n",
        "    )\n",
        "    return self.pooling(x)\n",
        "\n",
        "class Resnet_Like(keras.Model):\n",
        "  #keras.Model has added functionalities like built in training, evaluation added to the func.s of layers.Layer\n",
        "  #in the final model, keras.Model is to be used\n",
        "  def __init__(self, num_classes = 10):\n",
        "    super(Resnet_Like, self).__init__()\n",
        "    #specifying the channels for each of the CNN blocks\n",
        "    self.block1 = ResBlock([32,32,64])\n",
        "    self.block2 = ResBlock([128,128,256])\n",
        "    self.block3 = ResBlock([128,256,512])\n",
        "    self.pool = layers.GlobalAveragePooling2D()\n",
        "    self.classifier = layers.Dense(num_classes)\n",
        "\n",
        "  def call(self, input_tensor, training = False):\n",
        "    x = self.block1(input_tensor, training = training)\n",
        "    x = self.block2(x, training = training)\n",
        "    x = self.block3(x, training = training)\n",
        "    x = self.pool(x)\n",
        "    return self.classifier(x)\n",
        "  #function is to provide shape to the outputs in the model summary\n",
        "  def model(self):\n",
        "    x = keras.Input(shape = (28,28,1))\n",
        "    return keras.Model(inputs = [x], outputs = self.call(x))\n",
        "\n",
        "\n",
        "model = Resnet_Like(num_classes = 10)\n",
        "model.compile(\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics = [\"accuracy\"],\n",
        ")\n",
        "\n",
        "print(model.model().summary())\n",
        "\n",
        "model.fit(x_train, y_train, batch_size = 64, epochs = 1, verbose = 2)\n",
        "\n",
        "model.evaluate(x_test, y_test, batch_size = 64, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "res_block_3 (ResBlock)       (None, 14, 14, 64)        28640     \n",
            "_________________________________________________________________\n",
            "res_block_4 (ResBlock)       (None, 7, 7, 256)         526976    \n",
            "_________________________________________________________________\n",
            "res_block_5 (ResBlock)       (None, 3, 3, 512)         1839744   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 2,400,490\n",
            "Trainable params: 2,397,418\n",
            "Non-trainable params: 3,072\n",
            "_________________________________________________________________\n",
            "None\n",
            "938/938 - 1965s - loss: 0.0850 - accuracy: 0.9747\n",
            "157/157 - 77s - loss: 0.1506 - accuracy: 0.9560\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.15059037506580353, 0.9559999704360962]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHxFeekp6y3Z",
        "outputId": "b975c505-05c5-49da-f523-6499fef91393"
      },
      "source": [
        "#building Custom layers\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0 #this is being done to normalise the values of greyscale b/w 0 and 1, and float64 is being converted to float32 for ease in computation\n",
        "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "#building custom models\n",
        "class Dense(layers.Layer):\n",
        "  def __init__(self, units ): #input_dim\n",
        "    super(Dense, self).__init__()\n",
        "    self.units = units\n",
        "    \n",
        "  #build dim, if given, then there would be no need of paramete input_Weight\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(\n",
        "        name = 'w',\n",
        "        shape = (input_shape[-1], self.units),\n",
        "        initializer = 'random_normal',\n",
        "        trainable = True,\n",
        "    )\n",
        "\n",
        "    self.b = self.add_weight(\n",
        "        name = 'b',\n",
        "        shape = (self.units),\n",
        "        initializer = 'zeros',\n",
        "        trainable = True,\n",
        "    )\n",
        "\n",
        "  def call(self,inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "#custom ReLu function\n",
        "class MyReLu(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(MyReLu, self).__init__()\n",
        "\n",
        "  def call(self, x):\n",
        "    return tf.math.maximum(x,0)\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "  def __init__(self, num_classes = 10):\n",
        "    super(MyModel, self).__init__()\n",
        "    # self.dense1 = layers.Dense(64)\n",
        "    # self.dense2 = layers.Dense(num_classes)\n",
        "\n",
        "    #custom layers with input dimension and without build method\n",
        "    # self.dense1 = Dense(64,784)\n",
        "    # self.dense2 = Dense(10,64)\n",
        "\n",
        "    #with build method\n",
        "    self.dense1 = Dense(64)\n",
        "    self.dense2 = Dense(num_classes)\n",
        "    self.relu = MyReLu()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.relu(self.dense1(x))\n",
        "    return self.dense2(x)\n",
        "\n",
        "model = MyModel()\n",
        "model.compile(\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics = [\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size = 32, epochs = 2, verbose = 2)\n",
        "model.evaluate(x_test, y_test, batch_size = 32, verbose = 2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 - 3s - loss: 0.3436 - accuracy: 0.9071\n",
            "Epoch 2/2\n",
            "1875/1875 - 2s - loss: 0.1637 - accuracy: 0.9521\n",
            "313/313 - 0s - loss: 0.1426 - accuracy: 0.9587\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1425895392894745, 0.9587000012397766]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}